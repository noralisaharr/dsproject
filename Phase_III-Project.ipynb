{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa93144-0a7a-4a12-95e4-36b59d922d8e",
   "metadata": {},
   "source": [
    "# Phase III: First ML Proof of Concept (5\\%)\n",
    "\n",
    "### Team Names:\n",
    "- Luc\n",
    "- Nora\n",
    "- Kevin\n",
    "- Thomas\n",
    "- Zijiang Yang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51173382-263f-4249-baf7-56c4fe16fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all useful libraries (for webscraping and ML as well)\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861db69f-4357-4e66-8368-9507a9ef2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscraping\n",
    "api_key = \"03bcc17f7d105b13199e0325b659d4ab\"\n",
    "API_KEY = \"6c30ec8965840313bb630941ccabc149\"\n",
    "BASE_URL = \"https://api.themoviedb.org/3\"\n",
    "\n",
    "# Function to get movie details by ID\n",
    "def get_movie_details(movie_id):\n",
    "    \"\"\"\n",
    "    Gets the movie details of a movie through a movie id.\n",
    "\n",
    "    Args:\n",
    "        movie_id (string): the id of the movie as a string.\n",
    "    \n",
    "    Returns:\n",
    "        response: the reponse (as a json) recieved.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/movie/{movie_id}?api_key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching details for movie ID {movie_id}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to get movie credits by ID\n",
    "def get_movie_credits(movie_id):\n",
    "    \"\"\"\n",
    "    Gets the movie credits of a movie through a movie id.\n",
    "\n",
    "    Args:\n",
    "        movie_id (string): the id of the movie as a string.\n",
    "    \n",
    "    Returns:\n",
    "        response: the reponse (as a json) recieved.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/movie/{movie_id}/credits?api_key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching credits for movie ID {movie_id}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to get actor popularity by ID\n",
    "def get_actor_popularity(actor_id):\n",
    "    \"\"\"\n",
    "    Gets the popularity of an actor through an actor id.\n",
    "\n",
    "    Args:\n",
    "        actor_id (string): the id of the actor as a string.\n",
    "    \n",
    "    Returns:\n",
    "        response: the popularity of the actor.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/person/{actor_id}?api_key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('popularity')\n",
    "    else:\n",
    "      print(f\"Error fetching popularity for actor ID {actor_id}: {response.status_code}\")\n",
    "      return None\n",
    "\n",
    "page = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(50): #the number of pages to go up until\n",
    "    url = f\"{BASE_URL}/movie/top_rated?api_key={api_key}&language=en-US&page={page}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    new_data = pd.DataFrame(data['results'])\n",
    "\n",
    "    df = pd.concat([df, new_data])\n",
    "    page += 1\n",
    "\n",
    "# Filter out movies with vote count less than 1000\n",
    "df = df[df['vote_count'] >= 1000].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Add budget information\n",
    "df['budget'] = None\n",
    "for index, row in df.iterrows():\n",
    "    movie_id = row['id']\n",
    "    details = get_movie_details(movie_id)\n",
    "    if details and 'budget' in details:\n",
    "        df.loc[index, 'budget'] = details['budget']\n",
    "    time.sleep(0.05) # Add a small delay to avoid hitting API limits\n",
    "\n",
    "# Add top actor popularity\n",
    "df['top_actor_popularity (out of 10)'] = None # Initialize top_actor_popularity column\n",
    "for index, row in df.iterrows():\n",
    "    movie_id = row['id']\n",
    "    credits = get_movie_credits(movie_id)\n",
    "    if credits and 'cast' in credits and len(credits['cast']) > 0:\n",
    "        top_actor_id = credits['cast'][0]['id']\n",
    "        actor_popularity = get_actor_popularity(top_actor_id)\n",
    "        df.loc[index, 'top_actor_popularity (out of 10)'] = actor_popularity\n",
    "    time.sleep(0.05) # Add a small delay to avoid hitting API limits\n",
    "\n",
    "\n",
    "# Convertin budget to millions n dropping orginal budget column\n",
    "df['budget_in_millions'] = df['budget'] / 1000000\n",
    "df = df.drop(columns=['budget'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d7ec6e-cbfe-413b-b99f-bf6cef4cf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of cleaned January data (csv_files/cleaned_jan_202401-bluebikes-tripdata.csv):\n",
      "            ride_id  rideable_type           started_at             ended_at  \\\n",
      "0  D2F4A4783B230A84  electric_bike  2024-01-31 12:16:49  2024-01-31 12:21:02   \n",
      "1  D305CEFFD4558633   classic_bike  2024-01-12 08:14:16  2024-01-12 08:19:48   \n",
      "2  02009BB4EBA0D1F6  electric_bike  2024-01-29 15:00:05  2024-01-29 15:05:47   \n",
      "3  04C230C1C39071F7   classic_bike  2024-01-09 16:33:40  2024-01-09 17:00:41   \n",
      "4  CEAFE67E28B43852   classic_bike  2024-01-23 10:19:21  2024-01-23 10:31:39   \n",
      "\n",
      "   start_station_name start_station_id  \\\n",
      "0  Ames St at Main St           M32037   \n",
      "1  Ames St at Main St           M32037   \n",
      "2  One Memorial Drive           M32053   \n",
      "3  Ames St at Main St           M32037   \n",
      "4  Mass Ave T Station           C32063   \n",
      "\n",
      "                          end_station_name end_station_id  start_lat  \\\n",
      "0    Central Square at Mass Ave / Essex St         M32011  42.362357   \n",
      "1    Central Square at Mass Ave / Essex St         M32011  42.362500   \n",
      "2  Kennedy-Longfellow School 158 Spring St         M32065  42.361697   \n",
      "3                      Brookline Town Hall         K32005  42.362500   \n",
      "4                         Chinatown T Stop         D32019  42.341356   \n",
      "\n",
      "   start_lng    end_lat    end_lng member_casual  tripduration  \n",
      "0 -71.088163  42.365070 -71.103100        member      4.216667  \n",
      "1 -71.088220  42.365070 -71.103100        member      5.533333  \n",
      "2 -71.080273  42.369553 -71.085790        member      5.700000  \n",
      "3 -71.088220  42.333765 -71.120464        member     27.016667  \n",
      "4 -71.083370  42.352409 -71.062679        member     12.300000  \n"
     ]
    }
   ],
   "source": [
    "#clean data\n",
    "def get_languages():\n",
    "    \"\"\"\n",
    "    This function gets the list of movie languages from the TMDB.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): The list of languages as a dictionary.\n",
    "    \"\"\"\n",
    "    url = \"https://api.themoviedb.org/3/configuration/languages\"\n",
    "    params = {\"api_key\": api_key}\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def get_movie_genres():\n",
    "    \"\"\"\n",
    "    This funciton gets the list of movie genres from the TMDB.\n",
    "    \n",
    "    Returns:\n",
    "        response (dict): The list of genres as a dictionary.\n",
    "    \"\"\"\n",
    "    url = \"https://api.themoviedb.org/3/genre/movie/list?language=en\"\n",
    "    params = {\"api_key\": api_key}\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the data given into these metrics: \n",
    "        - id\n",
    "        - title\n",
    "        - popularity\n",
    "        - rating\n",
    "        - number_of_raters\n",
    "        - top_actor_popularity (out of 10)\n",
    "        - budget_in_millions\n",
    "        - release_year\n",
    "        - release_month\n",
    "        - release_day \n",
    "        - original language (as dummy variables in int (1) format)\n",
    "        - genres (as dummy variables in int (1) format)\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): the inputted dataframe to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        cleaned_df (DataFrame): a cleaned dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get the list of languages from TMDB\n",
    "    languages_dict = get_languages()\n",
    "    languages = {}\n",
    "    for lang in languages_dict:\n",
    "        languages[lang['iso_639_1']] = lang['english_name']\n",
    "    \n",
    "    #get the list of genres from TMDB\n",
    "    genres_dict = get_movie_genres()\n",
    "    genre_list = {}\n",
    "    for genre in genres_dict['genres']:\n",
    "        genre_list[genre['id']] = genre['name']\n",
    "\n",
    "    #initialize lists to hold data\n",
    "    movie_ids = []\n",
    "    movie_titles = []\n",
    "    movie_year = []\n",
    "    movie_month = []\n",
    "    movie_day = []\n",
    "    movie_language = []\n",
    "    movie_popularity = []\n",
    "    movie_genres = []\n",
    "    movie_vote_average = []\n",
    "    movie_vote_count = []\n",
    "    movie_budget = []\n",
    "    movie_top_actor_popularity = []\n",
    "\n",
    "    #loop through all rows of the data\n",
    "    for data in df.iterrows():\n",
    "        movie = data[1]\n",
    "\n",
    "        movie_ids.append(movie['id']) #append movie id\n",
    "\n",
    "        movie_titles.append(movie['title']) #append movie title\n",
    "\n",
    "        #append movie release date\n",
    "        movie_year.append(int(movie['release_date'][0:4])) \n",
    "        movie_month.append(int(movie['release_date'][5:7]))\n",
    "        movie_day.append(int(movie['release_date'][8:10]))\n",
    "\n",
    "        movie_language.append(languages[movie['original_language']]) #append movie language\n",
    "\n",
    "        movie_popularity.append(float(movie['popularity'])) #append movie popularity\n",
    "\n",
    "        #append movie genres as a list of genres\n",
    "        genres = movie['genre_ids']\n",
    "        list_of_genres = []\n",
    "        for index in genres:\n",
    "            list_of_genres.append(genre_list[index])\n",
    "        movie_genres.append(list_of_genres)\n",
    "\n",
    "        #append movie rating average and count\n",
    "        movie_vote_average.append(float(movie['vote_average']))\n",
    "        movie_vote_count.append(int(movie['vote_count']))\n",
    "\n",
    "        movie_budget.append(round(float(movie['budget_in_millions']), 3)) #append movie budget in millions (rounded to 3 decimal places)\n",
    "\n",
    "        movie_top_actor_popularity.append(movie['top_actor_popularity (out of 10)']) #append top actor popularity\n",
    "\n",
    "    #put all of the lists into a dictionary\n",
    "    df_dict = {'movie_id': movie_ids,\n",
    "               'title': movie_titles,\n",
    "               'popularity': movie_popularity,\n",
    "               'rating': movie_vote_average,\n",
    "               'number_of_raters': movie_vote_count,\n",
    "               'top_actor_popularity (out of 10)': movie_top_actor_popularity,\n",
    "               'budget_in_millions': movie_budget,\n",
    "               'genres': movie_genres,\n",
    "               'release_year': movie_year,\n",
    "               'release_month': movie_month,\n",
    "               'release_day': movie_day,\n",
    "               'original_language': movie_language,\n",
    "               }\n",
    "\n",
    "    cleaned_df = pd.DataFrame(df_dict) #create the dataframe\n",
    "\n",
    "    #make dummy variables for categorical data\n",
    "    cleaned_df = pd.get_dummies(cleaned_df, columns=['original_language'], dtype='int')\n",
    "\n",
    "    df_exploded = cleaned_df.explode('genres') #explode the genres column so each genre has its own row (we'll remove these extra rows later)\n",
    "    df_genre_dummies = pd.get_dummies(df_exploded['genres']) #get the dummy variables for the genres\n",
    "    cleaned_df = cleaned_df.join(df_genre_dummies.groupby(df_exploded.index).sum()) #put rows back together again\n",
    "\n",
    "    cleaned_df = cleaned_df.drop(columns=['genres']) #drop the genres column now, as we don't need it\n",
    "\n",
    "    cleaned_df = cleaned_df.sort_values(by='movie_id') #sort the dataframe by movie_id (this isn't really necessary)\n",
    "    return cleaned_df.reset_index(drop=True) #return the dataframe, and reset the index of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b0ad71-5f76-4f8c-8866-05aa4c880b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of cleaned July data (csv_files/cleaned_july_202407-bluebikes-tripdata.csv):\n",
      "            ride_id  rideable_type               started_at  \\\n",
      "0  5FA2A0E02EC53028   classic_bike  2024-07-11 02:07:46.443   \n",
      "1  5D49B9B78C826FD5   classic_bike  2024-07-19 14:07:03.620   \n",
      "2  5FEF6FE539C078FB  electric_bike  2024-07-02 09:06:08.296   \n",
      "3  709B58276144026B   classic_bike  2024-07-30 13:55:16.971   \n",
      "4  15755F52835908F5   classic_bike  2024-07-18 19:24:45.546   \n",
      "\n",
      "                  ended_at                      start_station_name  \\\n",
      "0  2024-07-11 02:20:34.180            Maverick St at Massport Path   \n",
      "1  2024-07-19 14:08:06.530       NCAAA - Walnut Ave at Crawford St   \n",
      "2  2024-07-02 09:12:52.886  Medford Sq - Riverside Ave at River St   \n",
      "3  2024-07-30 14:54:55.077       NCAAA - Walnut Ave at Crawford St   \n",
      "4  2024-07-18 19:42:54.574            Maverick St at Massport Path   \n",
      "\n",
      "  start_station_id                     end_station_name end_station_id  \\\n",
      "0           A32044         Maverick St at Massport Path         A32044   \n",
      "1           B32027    NCAAA - Walnut Ave at Crawford St         B32027   \n",
      "2           F32002     Tufts Sq - Main St at Medford St         F32003   \n",
      "3           B32027  Harvard Square at Mass Ave/ Dunster         M32018   \n",
      "4           A32044            Addison St at Saratoga St         A32054   \n",
      "\n",
      "   start_lat  start_lng    end_lat    end_lng member_casual  tripduration  \n",
      "0  42.367741 -71.033360  42.367741 -71.033360        member     12.795617  \n",
      "1  42.316902 -71.091946  42.316902 -71.091946        member      1.048500  \n",
      "2  42.417680 -71.108055  42.401697 -71.106128        member      6.743167  \n",
      "3  42.316902 -71.091946  42.373268 -71.118579        casual     59.635100  \n",
      "4  42.367741 -71.033360  42.385181 -71.015137        casual     18.150467  \n"
     ]
    }
   ],
   "source": [
    "# Print first 50 rows of cleaned data\n",
    "cleaned_df = clean_data(df)\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0d7eb-a38b-4c8a-bc48-3a8b5924519f",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "(3%) The implementation (using NumPy) of your first ML model as a function call to the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658ce461-e0be-4a7e-9ea5-2d0e48253ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import probplot, shapiro\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd6d8d-42f7-40af-8dde-bd51f5a0f427",
   "metadata": {},
   "source": [
    "### Attempt 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107ae40-35a8-4e0e-ac3b-37aef6434e7a",
   "metadata": {},
   "source": [
    "### Attempt 2 - Polynomial regression\n",
    "#### Creating the Design Matrix\n",
    "\n",
    "Using the `PolynomialFeatures` and `.fit_transform` functions to convert the `hour` ($x$) feature into an array (`X_hour_poly`) that includes columns corresponding to building a quartic model for `tripduration` ($y$) along the lines of: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29ec1c-98f2-444f-a108-74bfee42f952",
   "metadata": {},
   "source": [
    "### Attempt 3 - Polynomial Regression with added interaction terms and dummy variables\n",
    "#### Creating Fitting Model\n",
    "\n",
    "$$\n",
    "y = b_0 + b_1 x_1 + b_2 x_1^2 + b_3 x_1^3 + b_4 x_1^4 + b_5x_1x_3 + b_6x_2x_4 + b_7x_1x_4 + b_8x_2x_3\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$: trip duration\n",
    "- $x_1$: time of day\n",
    "- $x_2$: member or casual (1 or 0)\n",
    "- $x_3$: electric or classic bike (1 or 0)\n",
    "\n",
    "- Polynomial terms $b_0 + b_1 x_1 + b_2 x_1^2 + b_3 x_1^3 + b_4 x_1^4$\n",
    "- Interaction terms with dummy variables $b_5x_1x_3 + b_6x_2x_4 + b_7x_1x_4 + b_8x_2x_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0cf99-6f28-4531-b44a-a97fce6abd7f",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "(2%) A discussion of the preliminary results:\n",
    "   - This may include checking of assumptions, generated plots/tables, measures of fit, or other attributes of the analysis\n",
    "   - It does not have to be fully correct, but as a proof of concept must demonstrate that the group is close to completing the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b178c-cd70-4932-a6a2-fc0d60f70d46",
   "metadata": {},
   "source": [
    "## THESE ARE THE EXAMPLES GIVEN!!\n",
    "\n",
    "## Initial Approach\n",
    "The initial question we proposed for Machine Learning part is \"Can we predict the trip duration based on factors such as the time of day, user demographics, and starting/ending stations?\", and after further discussions and analysis, we identified we can try building the models based on a Linear Regression for 3 features separately and a Polynomial Regression for one of the features. However, as we moved forward in with the Linear Regression analysis, we got unexpectedly large values for both $MSE$ and $R^2$ for all 3 features. This made us reconsider our initial attempt and go with Polynomial Regression for one of the features (Time of Day). After constructing the model equation and using cross-validation, we received values of $MSE$ = 627.869 and $R^2$ = 0.0035, which showed worse results, compared to Linear Regression. Therefore, we decided to fix our model by incorporating interactions with dummy variables (member_casual and rideable_type), which had values of 1 and 0. Building the polynomial regression model with interactions and dummy variables has slighly improved the $MSE$ = 599.699 and $R^2$ = 0.0482. However, the values were still significantly worse, than the Linear Regression model's ones. Therefore, we we will use the Linear Regression model to discuss the preliminary results.\n",
    "\n",
    "## Model 1\n",
    "First, we analyzed Model 1: Time of Day of Linear Regression model. We calculated $MSE$ = 514.094 and $R^2$ = 0.0477, where MSE value is lower than for the polynomial regression models and $R^2$ value is slightly lower than the second polynomial regression model. Based on the first plot of Residuals vs Fitted, we can see that the assumption of Linearity and Constant Variance is violated as the residuals do not follow a linear trend about 0 and also contain multiple outliers. The second plot of Residuals vs Order shows that there is some density of the data points near the y = 0 with mutliple outliers as well, indicating there might be a violation of Independence assumption as well. Looking at the Histogram of Residuals and Q-q plot, we can witness there is strong right-skewedness, showing that there is no normal distribution, meaning violation of Normality assumption. \n",
    "\n",
    "## Model 2\n",
    "Second, we analyzed Model 2: User Demographics of Linear Regression Model. Calculating $MSE$ = 508.001 and $R^2$ = 0.059 shows that this is the highest values we were able to identify. There is a violation of Linearity and Constant Variance assumptions based on the Residuals vs Fitted graph, data points are not equally distributed around y=0 line, they have significant gaps between vertical line patterns of the data points. Moreover, the Independence assumption might be violated due to the density above y = 0 line. Finally, we see that the residuals are right skewed on Q-Q plot and Histogram of residuals. Therefore, the plots also violate the assumptions, despite better looking values.\n",
    "\n",
    "## Model 3\n",
    "Finally, the last Model 3: Starting and Ending Stations has $MSE$ = 514.076 and $R^2$ = 0.0478, the values are somewhat similar to the ones from Model 1. The model also fails to meet the assumptions of Linearity and Constant Variance assumptions based on the Residuals vs Fitted graph, data points are not distributed closer to y=0 line, have occasional gaps and outliers. Moreover, the Independence assumption might be violated due to the density above y = 0 line. Finally, we see that the residuals are right skewed on Q-Q plot and Histogram of residuals. \n",
    "\n",
    "## Conclusions\n",
    "These findings paint a complex picture of the bike-sharing prediction challenge. While Linear Regression provided our best results among the attempted approaches, the consistent violations of key assumptions across all models reveal fundamental challenges in our modeling strategy. \n",
    "\n",
    "First, the pervasive violations of regression assumptions across all models suggest systemic issues in how we're approaching the prediction task. The violation of linearity assumptions, evidenced by clear patterns in our residual plots, indicates that the relationship between our predictors and trip duration isn't as straightforward as our models assume. For instance, the impact of time of day on trip duration might follow a more complex pattern influenced by rush hours, weekends, or seasonal effects that our linear approach can't capture. Similarly, the relationship between station locations and trip duration might be influenced by geographic features, traffic patterns, or neighborhood characteristics that require more sophisticated modeling approaches.\n",
    "\n",
    "The heteroscedasticity observed in our models (non-constant variance in residuals) suggests that our predictions' accuracy varies significantly across different conditions. This could indicate that trip durations are more predictable in some circumstances than others – perhaps more consistent during regular commuting hours but more variable during leisure times. This varying predictability challenges our model's ability to provide reliable estimates across all scenarios.\n",
    "\n",
    "The notably low R² values (ranging from 0.0477 to 0.059) are particularly telling. These values indicate that our models explain less than 6% of the variance in trip durations, leaving over 94% of the variation unexplained. This substantial unexplained variance could stem from several sources, such as missing key features, complex interaction, or hidden variables.\n",
    "\n",
    "Future research directions might include:\n",
    "- Exploring more sophisticated modeling techniques such as mixed-effects models to account for hierarchical structure in the data\n",
    "- Investigating even more additional features that might better explain trip duration variability\n",
    "- Considering non-parametric approaches that don't rely on the strict assumptions of linear regression\n",
    "- Implementing geospatial modeling techniques to better capture the impact of station locations\n",
    "\n",
    " Understanding these limitations and potential improvements is crucial for developing more effective prediction models in future iterations. The complexity revealed by our analysis suggests that successful trip duration prediction might require a more nuanced, multi-model approach that can adapt to different conditions and user patterns. Our results demonstrate the challenges of real-world predictive modeling and the importance of thorough diagnostic testing, even when working with seemingly straightforward prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914cb96-3cbe-4b05-8b14-975b74be81c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
